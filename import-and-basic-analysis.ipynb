{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import textract\n","import fitz\n","import string\n","import re\n","import sys\n","import os\n","from os import system, listdir\n","import numpy as np\n","import pandas as pd\n","import collections\n","import pycountry\n","import PyPDF2\n","import scattertext as st\n","import spacy\n","import spacy_langdetect\n","from pycountry import languages\n","from spacy import matcher\n","from spacy import tokens\n","from spacy.matcher import Matcher\n","from spacy.tokens import Token\n","\n","from spacy_langdetect import LanguageDetector\n","\n","\n","# check if python and pip as well as the executable are in line\n","os.system('which python')\n","os.system('which pip')\n","sys.executable\n","\n","######## Define Functions ##############\n","\n","def pdf_to_text(path, extensions):\n","    \"\"\"transforms pdf and other files to txt format and safes it in the path folder\n","    adopted from PyMuPDF Tutorial\"\"\"\n","    for x in os.listdir(path):\n","        # Wenn es sich um ein pdf-File handelt, nehmen wir das paket PyMuPDF via fitz\n","        if x.endswith('.pdf'):\n","            #\"\"\"for i in range(0, len(filenames_list)): fname = filenames_list[i]\"\"\"\n","            doc = fitz.open(path+x)  # open document\n","            out = open(path+x[:-4] + \".txt\", \"wb\")  # open text output\n","            for page in doc:  # iterate the document pages\n","                text = page.getText().encode(\"utf8\")  # get plain text (is in UTF-8)\n","                out.write(text)  # write text of page\n","                # write page delimiter (form feed 0x0C)\n","                out.write(bytes((12,)))\n","            out.close()\n","        # Ansonsten probieren wir es mit textract, einem wrapper für verschiedene\n","        # Pakete, die verschiedene Dateien in Text-Dateien transformieren können.\n","        # Jedoch kann es hier auch unsauberen Output geben.\n","        elif x.endswith(tuple(extensions)) and not x.endswith('.pdf') and not x.endswith('.txt'):\n","            print(\n","                \"Will try to parse file {file} with other extension.\".format(file=x))\n","            try:\n","                text = textract.process(path+x)\n","                # xxxx could make -extensions\n","                #xclean = x.replace(extensions, \"\")\n","                out = open(path+x[:-4] + \".txt\", \"wb\")\n","                out.write(text)\n","                out.close()\n","                print(\"Sucessfully parsed\")\n","            except:\n","                print(\"Failed to parse document\")\n","\n","def get_textname(filename):\n","    textname = filename.split(\".txt\")[0]\n","    textname = textname.replace('_', ' ')\n","    return textname\n","\n","def import_doc(path, filename):\n","    \"\"\"simply importing the txtfiles\"\"\"\n","    title_path = (path+filename)\n","    with open(title_path, \"r\", encoding=\"utf8\", errors='ignore') as current_file:\n","        text = current_file.read()  # get it directly clean. optional if using nlpre!\n","        # text = clean_words(text)\n","    return text\n","\n","def clean_words(text, useful_characters):\n","    \"\"\"makes some cleaning, especially excluding characters that should not\n","    appear in text of the languages used in this script. Must be silence\n","    for Chinese, Russian and other languages \"\"\"\n","    text = re.sub(\"ß\", \"ss\", text)\n","    # remove weird characters\n","    text = ''.join(filter(lambda x: x in useful_characters, text))\n","    # remove double slashes and thus http//\n","    #text = re.sub(\"//|[\\\\\\]\", \" \", text)\n","    #text = re.sub(\" n\", \" \", text)\n","    # text = re.sub(\"|\\d\",\"\",text)#remove digits\n","    # text = re.sub(\"[\\\\\\]\", \"\", text)#remove backslashes\n","    # text = re.sub(\"\\s+\", \" \", text)  # remove empty parts like \"aa     bb.\"\n","    return text\n","    # see https://www.w3schools.com/python/python_regex.asp\n","\n","\n","def is_any_lowercase(sentence):\n","    \"\"\"\n","    Checks if any letter in a sentence is lowercase, return False if there\n","    are no alpha characters.\n","    Args:\n","        tokens: A list of string\n","    Returns:\n","        boolean: True if any letter in any token is lowercase\n","    \"\"\"\n","    if any(x.isalpha() & (x == x.lower()) for x in sentence):\n","        return True\n","    return not any(x.isalpha() for x in sentence)\n","\n","def titlecaps(doc):\n","    '''lowercases any sentence, where the entire sentence is written uppercase\n","    source: https://github.com/NIHOPA/NLPre/blob/master/nlpre/titlecaps.py'''\n","    # Need to keep the parser for sentence detection\n","    sents = doc.sents\n","    doc2 = []\n","    for sent in sents:\n","        line = sent.text\n","        if not is_any_lowercase(line):\n","            if len(line) > 4:\n","                line = line.lower()\n","        doc2.append(line + sent[-1].whitespace_)\n","    doc2 = \"\".join(doc2)\n","    return doc2\n","\n","def token_replacement(doc):\n","    \"\"\"replaces tokens some common symbols and characters with corresponding letters\n","    source: https://github.com/NIHOPA/NLPre/blob/master/nlpre/token_replacement.py\"\"\"\n","    replace_dict = {\n","        # these should not be used since they only work for english...\n","        # \"&\": \" and \",\n","        # \"%\": \" percent \",\n","        # \">\": \" greater-than \",\n","        # \"<\": \" less-than \",\n","        # \"=\": \" equals \",\n","        \"#\": \" \",\n","        \"~\": \" \",\n","        \"/\": \" \",\n","        \"\\\\\": \" \",\n","        \"|\": \" \",\n","        \"$\": \"\",\n","        \"€\": \" Euro\",\n","        # Remove empty :\n","        \" : \": \" \",\n","        # Remove double dashes\n","        \"--\": \" \",\n","        # Remove possesive splits\n","        \" 's \": \" \",\n","        # Remove quotes\n","        \"'\": \"\",\n","        '\"': \"\",\n","    }\n","    # for key in replace_dict:\n","    #    replace_dict[key] = \" \" #was macht das?????\n","    text = doc.text\n","    for key, val in replace_dict.items():\n","        text = text.replace(key, val)\n","        # Remove blank tokens, but keep line breaks\n","        text = [\n","            \" \".join([token for token in line.split()])\n","            for line in text.split(\"\\n\")\n","        ]\n","        # Remove blank lines\n","        text = \"\\n\".join(filter(None, text))\n","    return text\n","\n","def url_replacement(doc):\n","    \"\"\"replaces url text with blank\n","    source: https://github.com/NIHOPA/NLPre/blob/master/nlpre/url_replacement.py\"\"\"\n","    text = []\n","    for token in doc:\n","        if token.like_url:\n","            text.append(\"\")\n","            text.append(token.whitespace_)\n","        elif token.like_email:\n","            text.append(\"\")\n","            text.append(token.whitespace_)\n","        else:\n","            text.append(token.text_with_ws)\n","    return \"\".join(text)\n","\n","def keep_root(token, word_order=0):\n","    # If it's the first word, keep any other letters are capped\n","    # Otherwise, keep if any letters are capped.\n","    shape = token.shape_\n","    if word_order == 0:\n","        shape = shape[1:]\n","    return \"X\" in shape\n","\n","def pos_tokenizer(doc, nlp, POS_blacklist, maxlength, use_base=True):\n","    ''' Diese Funktion setzen wir erst ein, wenn wir bereits die Sprache kennen.\n","    Daher benutzen wir hier nicht das default englische nlp, sondern die Sprachenspezifische.\n","    Gerade für diese Funktion ist dies wichtig, da wir hier ja auf die token.pos_\n","    Variable gehen, die von der Sprache abhängt...\n","    source: https://github.com/NIHOPA/NLPre/blob/master/nlpre/pos_tokenizer.py'''\n","    #text = \" \".join(text.strip().split())\n","    special_words = set([\"_\"])\n","    doc = doc\n","    txt = []\n","    nouns = []\n","    verbs = []\n","    adjs = []\n","    with nlp.disable_pipes(\"ner\"):\n","        nlp.max_length = maxlength\n","        for sent in doc.sents:\n","            sent_tokens = []\n","            sent_nouns = []\n","            sent_verbs = []\n","            sent_adjs = []\n","            for k, token in enumerate(sent):\n","                # If we have a special word, add it without modification\n","                if any(sw in token.text for sw in special_words):\n","                    sent_tokens.append(token.text)\n","                    continue\n","                if token.pos_ in POS_blacklist:\n","                    continue\n","                word = token.text\n","                if (\n","                    use_base\n","                    # vllt muss \"k\" noch removed werden\n","                    and not keep_root(token, k)\n","                    and token.pos_ != \"PRON\"\n","                ):\n","                    word = token.lemma_\n","                # If the word is a pronoun, we need to use the base form, see\n","                # https://github.com/explosion/spaCy/issues/962\n","                if token.lemma_ == \"-PRON-\":\n","                    word = token.text.lower()\n","                sent_tokens.append(word)\n","                if token.pos_ in [\"NOUN\"]:\n","                    noun = token.text\n","                    sent_nouns.append(noun)\n","                if token.pos_ in [\"VERB\"]:\n","                    verb = token.text.lower()\n","                    sent_verbs.append(verb)\n","                if token.pos_ in [\"ADJ\", \"ADV\"]:\n","                    adj = token.text.lower()\n","                    sent_adjs.append(adj)\n","            # xxxx replace lists ev. with a dictionary\n","            txt.append(\" \".join(sent_tokens))\n","            for nn in sent_nouns:\n","                if nn not in nouns:\n","                    nouns.append(\"\".join(nn))\n","            for vrb in sent_verbs:\n","                if vrb not in verbs:\n","                    verbs.append(\"\".join(vrb))\n","            for dj in sent_adjs:\n","                if dj not in adjs:\n","                    adjs.append(\"\".join(dj))\n","            #verbs.append(\" \".join(sent_verbs))\n","            #adjs.append(\" \".join(sent_adjs))\n","        txt = \" \".join(txt)\n","        nouns = \" \".join(nouns)\n","        verbs = \" \".join(verbs)\n","        adjs = \" \".join(adjs)\n","        return txt, nouns, verbs, adjs\n","\n","def get_spacy_tokenizer(default_lingo, supported_languages, higher):\n","    '''returns the nlp function corresponding to the language of a doc/corpus'''\n","    if default_lingo in supported_languages:\n","        if higher == False:\n","            if default_lingo == \"German\":\n","                import de_core_news_sm\n","                nlp = de_core_news_sm.load()\n","            elif default_lingo == \"English\":\n","                import en_core_web_sm\n","                nlp = en_core_web_sm.load()\n","            elif default_lingo == \"Spanish\":\n","                import es_core_news_sm\n","                nlp = es_core_news_sm.load()\n","            elif default_lingo == \"Frensh\":\n","                import fr_core_news_sm\n","                nlp = fr_core_news_sm.load()\n","            elif default_lingo == \"Portuguese\":\n","                import pt_core_news_sm\n","                nlp = pt_core_news_sm.load()\n","            else:\n","                import it_core_news_sm\n","                nlp = it_core_news_sm.load()\n","        else:\n","            if default_lingo == \"German\":\n","                import de_core_news_md\n","                nlp = de_core_news_md.load()\n","            elif default_lingo == \"English\":\n","                import en_core_web_md\n","                nlp = en_core_web_md.load()\n","            elif default_lingo == \"Spanish\":\n","                import es_core_news_md\n","                nlp = es_core_news_md.load()\n","            elif default_lingo == \"Frensh\":\n","                import fr_core_news_md\n","                nlp = fr_core_news_md.load()\n","            elif default_lingo == \"Portuguese\":\n","                # there is no pt_md model\n","                import pt_core_news_sm\n","                nlp = pt_core_news_sm.load()\n","            else:\n","                # there is no it_md model\n","                import it_core_news_sm\n","                nlp = it_core_news_sm.load()\n","    else:\n","        print(\"NOT A SUPPORTED LANGUAGE!\")\n","    return nlp\n","\n","def decide_language_detection(path, supported_languages, default_lingo):\n","    from pycountry import languages\n","    print(\"Does your directory with the name \\n {path} \\n contain single documents written in more than one language?\\n Enter y for yes or n for no\".format(\n","        path=path))\n","    while True:\n","        try:\n","            multilanguage = input()\n","        except ValueError:\n","            print(\"Sorry, I didn't understand that.\")\n","            continue\n","        if multilanguage not in [\"y\", \"n\"]:\n","            print(\"You gave a wrong input. Try again\")\n","        else:\n","            print(\"Thank you for your input. Language Detection starting now\")\n","            break\n","    nlp = get_spacy_tokenizer(default_lingo, supported_languages, higher=False)\n","    return multilanguage, nlp\n","\n","\n","def get_language(multilanguage, doc, text, default_lingo, supported_lingos):\n","    if len(doc.text) >= 6 and doc.text.isdigit() == False:\n","        \n","        if multilanguage == \"n\":\n","            textlist = text\n","            langcode = doc._.language['language']\n","            if langcode != \"UNKNOWN\":\n","                langlist = languages.get(alpha_2=langcode).name\n","                if langlist not in supported_lingos:\n","                    langlist = default_lingo\n","                    print(\n","                        \"Language detection probably not successfull, using default language.\")\n","            else:\n","                langlist = default_lingo\n","                print(\n","                    \"Language detection not successfull, using default language.\")\n","        else:\n","            langlist = []  # the list of all unique detected languages\n","            textlist = []  # the list which contains the string of text for each unique languages\n","            removerow = []  # just an index of rows\n","            count_sents = 0\n","            doc = nlp(text)\n","            for sent in doc.sents:\n","                count_sents += 1\n","                langcode = sent._.language['language']\n","                if langcode != \"UNKNOWN\":\n","                    langname = languages.get(alpha_2=langcode).name\n","                    if langname in supported_lingos:\n","                        if langname not in set(langlist):\n","                            langlist.append(langname)\n","                            textlist.append(sent.text)\n","                        else:\n","                            for k in range(0, len(langlist)):\n","                                if langname == langlist[k]:\n","                                    textlist[k] += \" \"+sent.text\n","            print(\"sum of all sentences in this doc is\", count_sents)\n","            for i in range(len(langlist)):\n","                percentageoflingo = int(\n","                    100*len(textlist[i])/len(''.join(textlist)))\n","                if percentageoflingo < 2 and not len(textlist[i]) > 500:\n","                    removerow.append(i)\n","                    # or len(textlist[i]) < 50:\n","            # wir löschen also jenen Teil des Textes, der eine der obrigen Bedingungen erfüllt\n","            # reverse the order so that not the first elements\n","            # are deleted before the latter ones, which would cause errors\n","            for j in sorted(removerow, reverse=True):\n","                del langlist[j]\n","                del textlist[j]\n","        return langlist, textlist\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["######## start the pipeline :) ##############\n","if __name__ == '__main__':\n","    # this is required, otherwise we get weird languages for long and untidy documents\n","    supported_languages = [\"English\", \"German\",\n","                           \"Spanish\", \"Portuguese\", \"French\", \"Italian\"]\n","    default_language = \"English\"  # making English the default...\n","    useful_characters = string.printable + \\\n","        'äöüÄÖÜéÉèÈáÁàÀóÓòÒúÚùÙíÍìÌñÑãÃõÕêÊâÂîÎôÔûÛ'  # filtering the characters of the texts\n","    parsable_extensions = ['.csv', '.doc', '.docx', '.eml', '.epub', '.json',\n","                           '.msg', '.odt', '.ogg', '.pdf', '.pptx', '.rtf', '.xlsx', '.xls']\n","    \"\"\" '.gif', '.jpg', '.mp3', '.tiff', '.wav', '.ps', '.html' \"\"\"\n","    # the extensions which we try to parse to text\n","    maxlength = 2000000  # default would be 1m\n","    minlength = 100\n","    POS_blacklist = [\"PUNCT\", \"PART\", \"SYM\", \"SPACE\",\n","                     \"DET\", \"CONJ\", \"CCONJ\", \"ADP\", \"INTJ\", \"X\", \"\"]\n","    # the parsing functions in use\n","    parsers = [titlecaps, token_replacement, url_replacement]\n","    ######### Determining the directory from which to import documents #########\n","    workingdir = os.getcwd()\n","    workingdir = ''.join(workingdir)\n","    if not workingdir.endswith(\"/\"):\n","        workingdir = workingdir + \"/\"\n","    elif not workingdir.startswith(\"/\"):\n","        workingdir = \"/\" + workingdir\n","    print(\"Current working directory is {}. Do you want to analyse documents from your working directory? Press Enter if so. Otherwise indicate the desired subfolder from working directory\".format(os.getcwd()))\n","    while True:\n","        datenablage = input()\n","        \"\"\"datenablage = \"daten/neu\"\"\"\n","        # 1. zuerst den richtigen Pfad bestimmen\n","        if datenablage == \"\":\n","            path = workingdir\n","        else:\n","            if not datenablage.endswith(\"/\"):\n","                datenablage = datenablage + \"/\"\n","            path = datenablage\n","        try:\n","            filenames_list = [x for x in os.listdir(\n","                path) if x.endswith(\".txt\") or x.endswith(tuple(parsable_extensions))]\n","        except:  # falls Fehler kommt weil Nutzer sub-path eingegeben hat\n","            path = workingdir+datenablage\n","            try:\n","                filenames_list = [x for x in os.listdir(\n","                path) if x.endswith(\".txt\") or x.endswith(tuple(parsable_extensions))]\n","            # 2. Einen Fehler werfen wenn kein (sub)directory vorliegt. Neu starten\n","            except:\n","                print(\"Error occured! Probably not a directory. Try again\")\n","                continue\n","        # 3. den Prozess erneut starten, falls zu viele Dateien im directory sind\n","        if len(filenames_list) > 30:\n","                # including a performance upper limit\n","            print(\"Too many files: use directory with less documents!\")\n","            continue\n","        elif filenames_list == []:\n","            print(\"No documents detected. Try again\")\n","            continue\n","        else:\n","            print(\"Using path \", path)\n","            break\n","    # now we let the user determine if he wants to use the sentence-wise\n","    # language detection or the document-wise. The sentence-wise allows\n","    # to ignore parts of docs that contain text not of interest, such\n","    # as metadata in english for a german document or so\n","    multilanguage, nlp = decide_language_detection(\n","        path, supported_languages, default_language)\n","    nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pdf_to_text(path, parsable_extensions)\n","filenames_lst = [x for x in os.listdir(\n","    path) if x.endswith(\".txt\")]\n","# these are the lists we want to fill:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filenames = []\n","textnames = []\n","filenames_list = []\n","textnames_list = []\n","languagelist = []\n","textspre = []\n","\n","for i in range(len(filenames_lst)):\n","    filename = str(filenames_lst[i])\n","    textname = get_textname(filenames_lst[i])\n","    text = import_doc(path, filename)\n","    if len(text) > minlength:\n","        textspre.append(text)\n","    filenames_list.append(filename)\n","    textnames_list.append(textname)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with nlp.disable_pipes(\"ner\", \"tagger\"):\n","    nlp.max_length = maxlength\n","    docs = list(nlp.pipe(textspre))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[],"source":["#das braucht fucking 5+ minuten, wie ist das möglich??\n","counter = 0\n","texts = []\n","\n","for doc in docs:\n","    for funct in parsers:\n","        txt = funct(doc)\n","    language, text, = get_language(multilanguage, doc,\n","                                   txt, default_language,\n","                                   supported_languages)\n","    if type(language) == list:\n","        textnew = []\n","        for j in range(len(language)):\n","            filenames.append(filenames_list[counter])\n","            textnames.append(textnames_list[counter])\n","        languagelist += language\n","        texts += text\n","    elif type(language) == str:\n","        filenames = filenames_list\n","        textnames = textnames_list\n","        languagelist.append(language)\n","        for funct in parsers:\n","            text = funct(doc)\n","        texts.append(text)\n","    else:\n","        print(\"something went wrong\")\n","    counter +=1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["doc_list = list(\n","    zip(filenames, textnames, languagelist, texts))\n","\n","doc_list.sort(key=lambda doc_list: doc_list[2])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filteredtxts = []\n","filteredADJss = []\n","filteredNOUNss = []\n","filteredVERBss = []\n","uniquelst = []\n","\n","for lin in sorted(list(set([row[2] for row in doc_list]))):\n","    nlp = get_spacy_tokenizer(\n","        lin, supported_languages, higher=False)\n","    texte = []\n","\n","    for i in range(len(languagelist)):\n","        if languagelist[i] == lin:\n","            texte.append(texts[i])\n","            \n","    docs = list(nlp.pipe(texte))\n","    filteredtexts = []\n","    filteredNOUNs = []\n","    filteredVERBs = []\n","    filteredADJs = []\n","    unique = []\n","\n","    for doc in docs:\n","        filteredtxt, filteredNOUN, filteredVERB, filteredADJ = pos_tokenizer(\n","            doc, nlp, POS_blacklist, maxlength)\n","        filteredtexts.append(filteredtxt)\n","        filteredNOUNs.append(filteredNOUN)\n","        filteredVERBs.append(filteredVERB)\n","        filteredADJs.append(filteredADJ)\n","        uniques = []\n","        for item in doc.ents:\n","            if item.text not in uniques:\n","                uniques.append(item.text)\n","        unique.append(uniques)\n","        #maybe I should only add the ents if they belong to the nlp.vocab, because a.t.m \n","        #I get wild ents for texts with wild strings.\n","        #downside would be that then I have to use the \"md\"-spacy-model for this function\n","\n","    filteredtxts += filteredtexts\n","    filteredNOUNss += filteredNOUNs\n","    uniquelst += unique\n","    filteredVERBss += filteredVERBs\n","    filteredADJss += filteredADJs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_doclist = pd.DataFrame(doc_list, columns=[\n","        'File', 'Textname', 'Sprache', 'Text'])\n","df_doclist['bereinigter Text'] = filteredtxts\n","df_doclist['Substantive'] = filteredNOUNss\n","df_doclist['Verben'] = filteredVERBss\n","df_doclist['Adjektive'] = filteredADJss\n","df_doclist['Entitäten'] = uniquelst"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(df_doclist.shape)\n","df_doclist.to_pickle(path+\"/df_doclist.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import de_core_news_sm\n","nlp = de_core_news_sm.load()\n","corpus = st.CorpusFromPandas(\n","    df_doclist, category_col='Sprache', text_col='bereinigter Text', nlp = nlp).build()\n","#actually, the category should be sth else than the language, because if we take the language we don't get a big overlap between the categories! so ideally we would train a textcat model BEFORE and use the category-column inside pd_doclist as the category"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pprint\n","from pprint import pprint\n","term_freq_df = corpus.get_term_freq_df()\n","term_freq_df['German words'] = corpus.get_scaled_f_scores('German')\n","pprint(list(term_freq_df.sort_values(by='German words', ascending=False).index[:20]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["html = st.produce_scattertext_explorer(\n","    corpus,\n","    category='German', #not_category_name='English',\n","    minimum_term_frequency=5, metadata=corpus.get_df()['Textname'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fn = path+path.split(\"/\")[-2]+'-Auswertung.html'\n","open(fn, 'wb').write(html.encode('utf-8'))\n","print('Open ' + fn + ' in Chrome or Firefox.')"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":2}